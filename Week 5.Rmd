---
title: "Homework 5"
author: "Samikshya"
date: "2022-10-31"
output: html_document
---
Chapter 7: More on Confidence Interval
## Question 7.1 

a. This is False because the probability that a cow produces on average between the given range is either 0 or 1. It either produces the milk within the given range (22-30)kg/day or it does not.
b) False because we cannot apply the confidence interval to every sample. It just means that  95% of the sample will include within its confidence interval the true population mean. 
c) This is false because we cannot claim that each sample will include the mean of the population 95% of the time. What we can say is 95% of the confidence intervals generated by each sample will contain the true mean. 
d) False. IT necessarily isnot within the interval.  
e) False, we can claim that for 95% of the sample collected,95% of these sample distribution intervals will include the true population mean. 


```{r}
library(tidyverse)
library(ggplot2)
library(resampledata)
library(dplyr)
```

## Question 7.4 
a)
Given sample has a t distribution so we can use the t distribution table. Here the n given determines the degrees of freedom that will determine the t confidence interval of the 90%. 

```{r}
# To find the quantile q of 90% with n = 5,15,30,100:
#The degrees of freedom is n-1 so df would be 4,14,29,99 respectively
df4 <- qt(0.95, 4)
df14 <- qt(0.95, 14)
df29 <- qt(0.95, 29)
df99 <- qt(0.95, 99)

print(df4)
print(df14)
print(df29)
print(df99)

abs(qnorm(0.95))  

```
The quantile variables are 2.12,1.76, 1.69. 1.66 for n value of 5,15,30,100 respectively. As n increases, we see that the q values decreases. This is because, as the n increases, the uncertainity decreases and the t value will closely follow the z score of the normal distribution. Their quantile value also comes closer to the z score of the normal distribution(1.281)
b)

```{r}
t.score90 = qt(0.95, df=14)
t.score95 = qt(p=0.975, df=14)
t.score99 = qt(p=0.995, df=14)

t.score90
t.score95
t.score99 

```
The t score used for n is 15 or df = 14 is listed above. We see that the t score value increases as the p value increases. So, as p increases, the confidence interval value reduces. This is because to increase more confince, we reduce the margin on error. 
## Question 7.5 

We know that the formula for margin of error is q(sd/sqr(n)). At given state, q is 1.86 for 95% percentile. So if we want to half this, the probability is 97.5. Now if we want to reduce the MOE than our formula becomes 1.96(sd/sqr(n2)) = 1.96*(sd/sqr(n1))/2
From algebraic calculation, sqr(n2) = 2 * sqr (n1), Squaring both sides, we get, n2 = 4 times n1.we can conclude that the new sample needs to be at least 4 times the original sample. 

7.5b)
## Question 7.6
Mean is 18.05, n =20, SD = 5. Confidence interval: 90% 
```{r}
n <- 20 
xbar <- 18.05
sd <- 5
percentage <- 0.90
zScore <- qt(percentage, n -1)
zScore
```


```{r}
Lowerlimit <- xbar - (zScore* ((sd/sqrt(n))))
Lowerlimit
Upperlimit <- xbar + (zScore* ((sd/sqrt(n))))
Upperlimit 
```


From the calculation above, the 90% confidence interval for the mean amount of sugar in half cup serving is 16.56, 19.53)



## Question 7.7
n = 100, mean = 120 hours, SD = 12.
Moderate skewness: so we use T distribution. 
To find, one sided lower t confidence 
Confidence interval = 95% 

```{r}
n <- 100 
xbar <- 120
sd <- 12
percentage <- 0.95
zScore <- qt(percentage, n -1)
zScore

Lowerlimit <- xbar - (zScore* ((sd/sqrt(n))))
Lowerlimit
```

The lower bound is 118.0075.

##Question 7.11

```{r}
data("Spruce")
ggplot(data = Spruce, mapping = aes(x = Ht.change))+
  geom_histogram(bins = 20)

```
The data looks highly skewed.

```{r}
qqnorm(Spruce$Ht.change,
         ylab = "Height Change in Spruce")

qqline(Spruce$Ht.change)
```
The dataset is skewed towards the right tail so we can use t tail test for better estimation. 

```{r}
t.test(Spruce$Ht.change, conf.level = 0.95)
```
The confidence interval is 28.336 and 33.529. Wecan say with 95% confidence that the mean of the Ht, Change will lie within the range of 28.336 and 33.5298. 

## Question 7.14

```{r}
data("Girls2004")
#glimpse(Girls2004)
```
```{r}
Smokers <- Girls2004 %>%
  filter(Smoker == "Yes")
#glimpse(Smokers)
```
```{r}
ggplot(data = Smokers, mapping = aes(x = Weight))+
  geom_histogram(bins = 20)
```
```{r}
qqnorm(Smokers$Weight,
         ylab = "Height Change in Spruce")

qqline(Smokers$Weight)
```
The weight is fairly normally distributed. 

For non-smokers
```{r}
NonSmokers <- Girls2004 %>%
  filter(Smoker == "No")
ggplot(data = NonSmokers, mapping = aes(x = Weight))+
  geom_histogram(bins = 20)
qqnorm(NonSmokers$Weight,
         ylab = "Height Change in Spruce")

qqline(NonSmokers$Weight)
```
The distribution of the non smokers is normally distributed with long right tail towards the end. 

Counting the number of smokers and nonsmokers: 
```{r}
Nsmokers = nrow(Smokers)
Nnonsmokers = nrow(NonSmokers)
Nsmokers
Nnonsmokers
```
There is discrepancy in numbers of sample between two groups as numbers of smokers is 11 and no of non smokers is 69. 

Conducting mean height difference with the babies born to smokers and non smokers with lower bound: 
```{r}
Weight_difference <- t.test(Weight~Smoker, data = Girls2004,alt = "greater",conf.level = 0.95 )
Weight_difference
```
The average weight difference between smokers and non-smokers lower bound is -14.991. We can say with 95% confidence that difference in weight between girls born to smokers and girls born to non-smokers is 14g higher than those with smokers mom. 

## Question 7.15

```{r}
data("Spruce")
#glimpse(Spruce)
```


```{r}
ggplot(data = Spruce, mapping = aes(x = Ht.change))+
  geom_histogram()+
  facet_wrap(~Fertilizer)
```

Qqplor for Weights where fertilizer is used

```{r}
data("Spruce")
#glimpse(Spruce)
qqnorm(Spruce$Ht.change[Spruce$Fertilizer =="F"],
         ylab = "Height Change in Spruce with Fertilizer")
qqline(Spruce$Ht.change[Spruce$Fertilizer =="F"])
```
The qq plot for fertilized sample show that the Height change for fertilized plants shows fairly normal distribution with tails in the end. The graph also shows that the data set is granular(gap in the values). This can also  be observed in the histogram.  

qq plot for non fertilized: 

```{r}
qqnorm(Spruce$Ht.change[Spruce$Fertilizer =="NF"],
         ylab = "Height Change in Spruce with No Fertilizer")
qqline(Spruce$Ht.change[Spruce$Fertilizer =="NF"])
```
The non fertilized pot also show similar pattern to the fertilzied pot in terms for data gap. However, the tail seems not to sway awat from the qq line compared to the fertilized ones. 


```{r}
Height_difference <- t.test((Spruce$Ht.change[Spruce$Fertilizer =="F"]),(Spruce$Ht.change[Spruce$Fertilizer =="NF"]), alt = "greater",conf.level = 0.95 )
Height_difference
```



The lower bound for the confidence interval of the height difference between fertilized and non fertilized is 11.466.We are 95% confident that the difference in height between fertilized and non fertilized is at least 11.47. 

If it is lower t confidence, we use greater. 

##Question 7.21 

#For exponential distribution:
#For n is 10
```{r}
n <- 10
N <- 10^4
t.sim <- numeric(N)

for (i in 1: N)
{
  x<- rexp(n,.2)
  
  t.sim[i] <- (mean(x) - 5)/(sd(x)/sqrt(n))
}

ggplot(data = tibble(t.sim), mapping = aes(x = t.sim))+
  geom_histogram(mapping = aes(y = ..density..))+
  stat_function(fun = dt, args = list(df = n-1))
```
#For n= 50 
```{r}
n <- 50
N <- 10^4
t.sim <- numeric(N)

for (i in 1: N)
{
  x<- rexp(n,.2)
  
  t.sim[i] <- (mean(x) - 5)/(sd(x)/sqrt(n))
}

ggplot(data = tibble(t.sim), mapping = aes(x = t.sim))+
  geom_histogram(mapping = aes(y = ..density..))+
  stat_function(fun = dt, args = list(df = n-1))
```
#n = 1000
```{r}
n <- 1000
N <- 10^4
t.sim <- numeric(N)

for (i in 1: N)
{
  x<- rexp(n,.2)
  
  t.sim[i] <- (mean(x) - 5)/(sd(x)/sqrt(n))
}

ggplot(data = tibble(t.sim), mapping = aes(x = t.sim))+
  geom_histogram(mapping = aes(y = ..density..))+
  stat_function(fun = dt, args = list(df = n-1))
```
When n is small, while the data seems to follow the normal distribution vaguely, especially towards the tail of the t line.The graph and the t distribution donot align towards the end.  However, as n increases, we can see that the distribution , it closely follows that the t distribution. 


FOr Uniform Distribution: 

n= 15:

```{r}
n <- 15
N <- 10^4 
xbar <- 7
uniform.sim <- numeric(N)

for (i in 1: N)
{
  x <- runif(n, min = 4, max = 10)
  uniform.sim[i] <- (mean(x) - xbar)/(sd(x)/sqrt(n))
}
```


```{r}
ggplot(data = tibble(uniform.sim), mapping = aes(x = uniform.sim))+
  geom_histogram(mapping = aes(y = ..density..))+
  stat_function(fun = dt, args = list(df = n-1))
```

n= 75

```{r}
n <- 75
N <- 10^4 
xbar <- 7
uniform.sim <- numeric(N)

for (i in 1: N)
{
  x <- runif(n, min = 4, max = 10)
  uniform.sim[i] <- (mean(x) - xbar)/(sd(x)/sqrt(n))
}
```


```{r}
ggplot(data = tibble(uniform.sim), mapping = aes(x = uniform.sim))+
  geom_histogram(mapping = aes(y = ..density..))+
  stat_function(fun = dt, args = list(df = n-1))

```
For n = 500

```{r}

n <- 500
N <- 10^4 
xbar <- 7
uniform.sim <- numeric(N)

for (i in 1: N)
{
  x <- runif(n, min = 4, max = 10)
  uniform.sim[i] <- (mean(x) - xbar)/(sd(x)/sqrt(n))
}
```


```{r}
ggplot(data = tibble(uniform.sim), mapping = aes(x = uniform.sim))+
  geom_histogram(mapping = aes(y = ..density..))+
  stat_function(fun = dt, args = list(df = n-1))
```
Similarly, for uniform distribution, the distribution closely starts following the t distribution line as the n increases.




## Question 7.27 
n = 700.
Break out into hives = 34 of 350 
Places hives = 56 of 350 
```{r}
p1.hives = 34/350
p1.hives
p2.placebohives = 56/350
p2.placebohives
```
proportion of those who took drugs and had hives is 0.0971
proportion of those who didnot take drugs(i.e placebo)  and had hives is 0.16

a)
```{r}
prop.test.hives<- prop.test(34,350, conf.level =0.95 )
prop.test.hives
```
With the group that had hives, the confidence interval is 0.0691,0.13429)
We can say with 95% confidence that the proportion of student taking the drug who break out in hives is (0.069,0.134), i,e 6.9%,13.4%

b) 

```{r}
prop.test.placebohives<- prop.test(56,350, conf.level =0.95 )
prop.test.placebohives
```
With the group that had hives, the confidence interval is 0.1240,0.2036)
We can say with 95% confidence that the proportion of student who didnot take the drugs and were in placebo drug who break out in hives is (0.124,0.204)i.e (12.4%,20.4%)

c) 
Yes, the interval overlaps. However, the overlap is only of 0.01 which is really small to really make a conclusion. 

d) 
Confidence interval in proportions
of students who break out in hives by using or not using this drug
```{r}
difference.hives <- prop.test(c(34,56), n= c(350,350), conf.level = 0.95)
difference.hives 
```
The difference in hives rate between those who took the drug and placebo is -0.115-0.0106. Since both the values are negative, we can say that proportion of those who take the drug will experiences  hives less by (11.5, 1.06) percent. 

e) I believe that the correct approach would be the one in d because here we are conducting a two sample proportion test just on drug effect between two groups, so comparison is occurring between two groups we are interested in. Through this, the skewness of each other could cancel to create a balanced one. 
The approach on c, because we are making comparison across two different test, we are introducing more variables and maybe more error and skewness without getting new benefits. We have to infer the answers from two different test to create a conclusion. 

## Question 7.32

```{r}
data(MobileAds)
#glimpse(MobileAds)

```

a)
```{r}
qqnorm(MobileAds$m.cost_pre,
         ylab = "Cost Pre Experiment")
qqline(MobileAds$m.cost_pr)
```
The graph has a long tail to the right that wont follow normal distribution. It looks like a quadratic equation. 


```{r}
qqnorm(MobileAds$m.cost_post,
         ylab = "Cost Post Experiment")
qqline(MobileAds$m.cost_post)
```
Similar pattern.  


b)It is not appropriate to two sample interval because it is a match pais. Both pre and post cost are related to one another and will affect the behavior/outcome of the experiment. 

```{r}
N <- nrow(MobileAds)
diff.cost <- numeric(N) 

for (i in 1:N)
{ pre <-MobileAds$m.cost_pre
  post <-MobileAds$m.cost_post
  diff.cost[i] <- pre[i]-post[i]
  
  }


```


```{r}
qqnorm(diff.cost,
         ylab = "Cost Post Experiment")
qqline(diff.cost)
```
The difference between two variables follows normal distribution with tails on both sides.

d)

```{r}
diff<- t.test(diff.cost, conf.level = 0.95)
diff

``` 

The 95% confidence interval is (0.2086,27.334). So we can say with 95% confidence that true mean change as a result of the experiment will lie within the range of (0.2086,27.334) 

e)

```{r}

N <- 10^4-1
size.data <- length(diff.cost)
diff.prepostcost <- numeric(N)
for(i in 1:N)
{
  precost <- sample(diff.cost, size = size.data, replace = TRUE)
   diff.prepostcost[i] <- (mean(precost) - mean(diff.cost))/(sd(diff.cost)/sqrt(size.data)) 
}
```


```{r}
mean(diff.cost) - quantile(diff.prepostcost, c(0.975,0.025))*sd(diff.cost)/sqrt(size.data)
```
The confidence interval is (0.5996,27.222). This value range is very similar to what we calculated at d)

```{r}
